# -*- coding: utf-8 -*-
"""DataPreprocessing_Assignment11 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EnClXkumoWNDJAtRrtSlea6-0an1VSNc
"""

#Data.csv

"""**Step 1: Importing the libraries**"""

import pandas as pd
import numpy as np

"""**Step 2: Importing dataset**"""

df=pd.read_csv("/content/Data.csv")
df

"""**Step 3: Handling the missing data**"""

df.isnull()

df['Salary']=df['Salary'].fillna(df['Salary'].mean())
df['Age']=df['Age'].fillna(df['Age'].mean())
df

"""**Step 4: Encoding categorical data**"""

from sklearn.preprocessing import OneHotEncoder
df_np=np.array([['France'],['Spain'],['Germany']])
df_np

df1=pd.DataFrame(df_np,columns=['Country'])
df1

OHE=OneHotEncoder(sparse=False)
df1=OHE.fit_transform(df1)
type(df1)

df1=pd.DataFrame(df1)
df1

"""**Step 5: Creating a dummy variable**"""

df

df_dc=pd.get_dummies(df,columns=['Country'])
df_dc

X=df_dc.loc[:,['Age','Salary','Country_France','Country_Germany','Country_Spain']]
X

y=df_dc.loc[:,['Purchased']]
y

"""**Step 6: Splitting the datasets into training sets and Test sets**"""

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=0)

"""**Step 7: Feature Scaling**"""

from sklearn.preprocessing import StandardScaler

SS=StandardScaler()
X_train=SS.fit_transform(X_train)
X_train